{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#coding=utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib 為收集了多個用到URL的套件:\n",
    "    # urllib.request打開和讀取URL\n",
    "    # urllib.error包含urllib.request拋出的異常\n",
    "    # urllib.parse用於解析URL\n",
    "    # urllib.robotparser用於解析robots.txt文件\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# 一個HTML解析器，可以處理各種奇怪的HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# os 套件提供了非常豐富的方法用來處理文件和目錄\n",
    "import os\n",
    "\n",
    "# json.dumps 用於將 Python 對象編碼成 JSON 字符串\n",
    "import json\n",
    "\n",
    "# 建立各種 HTTP 請求，從網頁伺服器上取得想要的資料\n",
    "import requests\n",
    "\n",
    "# 使 Python 語言擁有全部的正則表達式功能\n",
    "import re\n",
    "\n",
    "# 存檔成CSV用 \n",
    "import pandas as pd\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先建立所有連結的集合set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目標: 先搜尋1 to 1900 (搜尋值)，將每頁\"食物名\"連結中各\"食用量\"的連結中的營養成分做成set() (集合,內文字為唯一值)\n",
    "\n",
    "流程：\n",
    "\n",
    "    第一部分：得所有第一層之連結（頁數連結）。\n",
    "        1. 建立\"頁數\"連結 = 空set()。\n",
    "        2. 做個搜尋值迴圈(while:)得各頁數連結 \n",
    "            **使用if else做無效搜尋職或頁數之停損點。\n",
    "        4. 將上述順序倒過來迴圈，並加入set()。\n",
    "        5. 將set()轉換為list()，供第二部分使用。\n",
    "        6. 將set()使用sorted依順序後存檔（csv）。\n",
    "    \n",
    "    第二部分：得第二層\"食物名\"連結。\n",
    "        1. 建立\"食物名\"連結 = 空set()\n",
    "        2. 使用for迴圈，從\"頁數\"set()得\"食物名\"連結。\n",
    "        3. 將set()使用sorted依順序後存檔（csv），供第三部分使用。\n",
    "                    \n",
    "    第三部分：得第三層可能隱藏之連結。\n",
    "        1. if:各\"食物名\"連結內有各\"食用量\"之連結，則得各\"食用量\"連結，else:僅得預設連結(可用len(網頁)為判別。並加入set()\n",
    "            **因某些食物名連結為error: 做try except以跳過無效連結。\n",
    "        2. 將set()使用sorted依順序後存檔（csv），以供第四部份使用。\n",
    "    \n",
    "    第四部分：爬蟲。\n",
    "        1. 創字典。\n",
    "        2. 將list()，以迴圈將品牌、食物名、營養成分、資料來源等，放入字典。        \n",
    "        \n",
    "    第五部分：存檔。\n",
    "        1. 將不符合資料夾、檔案命名格式之文字轉換為\"\"。\n",
    "        2. 將食物名、營養成分、資料來源設成json格式儲存至指定資料夾。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：得所有第一層之連結。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 網頁原本的樣子 https://www.fatsecret.cn/%E7%83%AD%E9%87%8F%E8%90%A5%E5%85%BB/search?q=0&pg=0\n",
    "mobile = \"https://mobile.fatsecret.cn/%E7%83%AD%E9%87%8F%E8%90%A5%E5%85%BB/search?q=\"\n",
    "website = \"https://www.fatsecret.cn/%E7%83%AD%E9%87%8F%E8%90%A5%E5%85%BB/search?q=\"\n",
    "pg = \"&pg=\"\n",
    "pageLinks_set = set()\n",
    "pageLinks_csv = [] # 將起始\"全部頁數連結\"為空字串\n",
    "\n",
    "################## 設數字 #########################\n",
    "q = 0 #設搜尋起始數字\n",
    "count = 100\n",
    "##################################################\n",
    "\n",
    "print(\"開始抓第一部分：第一層之所有連結\")\n",
    "\n",
    "while True:\n",
    "    pgNumber = 0 #設每一搜尋值之起始頁數\n",
    "    \n",
    "    # 超過搜尋頁數時，網頁會跳轉成 \"mobile\" 手機版網頁，所以先以此做為檢查依據\n",
    "    web = mobile + str(q) + pg + str(pgNumber) \n",
    "    response = requests.get(web)  # 輸入cookie及內容\n",
    "    html = BeautifulSoup(response.text) # 如果是requests, 多一個步驟, .text欄位拿出\n",
    "\n",
    "    if count > 101: # 防止無限迴圈 # 當累積\"超過頁數\"3次時，表示搜尋值及頁數皆到盡頭，沒東西可抓了\n",
    "        print(\"無搜尋值：\" + str(q-100) + \"到底了！\")\n",
    "        pageLinks_csv.append((\"無搜尋值：\" + str(q-100) + \"到底了！\" , \" \"))\n",
    "        break   \n",
    "\n",
    "    # 若該搜尋值僅有1頁，抓連結外，直接將搜尋值+1，回外層迴圈\n",
    "    elif len(html.find_all(\"td\", align=\"center\")) == 0:\n",
    "        print(\"搜尋值：\" + str(q) + \"，僅有1頁\")\n",
    "\n",
    "        websiteLink = website + str(q) + pg + str(pgNumber) \n",
    "        pageLinks_set.add(websiteLink)\n",
    "        pageLinks_csv.append((\"搜尋值：\" + str(q) + \"，僅有1頁：\", websiteLink))\n",
    "        print(websiteLink)\n",
    "        print(\"\")\n",
    "        \n",
    "        count = 0\n",
    "        q += 1\n",
    "                \n",
    "    else:\n",
    "        while True:\n",
    "            # 抓該頁連結連結\n",
    "            web = mobile + str(q) + pg + str(pgNumber) \n",
    "            response = requests.get(web)  \n",
    "            html = BeautifulSoup(response.text) \n",
    "\n",
    "            # 如果超過該搜尋值之頁數則break，回外層迴圈\n",
    "            if ((html.find_all(\"td\", align=\"center\")[0].text).split('\\\"')[0])  == '没有找到':\n",
    "                print(\"搜尋值:\" + str(q) + \"並無第\"+ str(pgNumber+1) + \"頁，超過該搜尋值之頁數\")\n",
    "                print(\"\")\n",
    "                \n",
    "                q += 1\n",
    "                count += 1\n",
    "                break                \n",
    "            \n",
    "            # 進此，表示該搜尋值有其他頁\n",
    "            else:\n",
    "                print(\"搜尋值：\"+ str(q) +\"，第\" + str(pgNumber) +\"頁\" )     \n",
    "                websiteLink = website + str(q) + pg + str(pgNumber) \n",
    "                \n",
    "                print(websiteLink)\n",
    "                pageLinks_set.add(websiteLink)\n",
    "                pageLinks_csv.append((\"搜尋值：\"+ str(q) + \"頁數：\" + str(pgNumber) + \"：\", websiteLink))\n",
    "                \n",
    "                count = 0\n",
    "                pgNumber += 1\n",
    "                \n",
    "\n",
    "pageLinks_set = sorted(pageLinks_set) # 依順序排序\n",
    "pageLinks_csv = sorted(pageLinks_csv)\n",
    "\n",
    "# 儲存檔案\n",
    "# 如果沒有該檔案名，創資料夾\n",
    "dn = \"fatsecret_food\"\n",
    "if not os.path.exists(dn):   \n",
    "    os.makedirs(dn)    \n",
    "\n",
    "# 將pageLinks_scv存檔\n",
    "print(\"頁數連結存檔中...\")\n",
    "\n",
    "my_df = pd.DataFrame(pageLinks_set)\n",
    "my_df.to_csv(dn + \"/\" + 'WebLinks_fatsecret_1_pageLinks_set', index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "my_df = pd.DataFrame(pageLinks_csv)\n",
    "my_df.to_csv(dn + \"/\" + 'WebLinks_fatsecret_1_pageLinks_scv', index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"頁數連結存檔（csv）完成\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分：得第二層\"食物名\"連結。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodLinks_set = set() # 建立\"食物名\"連結集合\n",
    "foodLinks_csv = set()\n",
    "\n",
    "for link in pageLinks_set:\n",
    "    \n",
    "    pageLinks = link\n",
    "    response = requests.get(pageLinks)\n",
    "    html = BeautifulSoup(response.text) \n",
    "    html_foodLinks = html.find_all(\"a\", class_ = \"prominent\")\n",
    "\n",
    "    for i in range (len(html_foodLinks)):\n",
    "        foodName = html_foodLinks[i].text\n",
    "        foodLinks = \"https://www.fatsecret.cn/\" + html_foodLinks[i][\"href\"]\n",
    "        foodLinks_set.add((foodLinks))\n",
    "        foodLinks_csv.add((foodName, foodLinks))\n",
    "        print(foodName)\n",
    "        print(foodLinks)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "foodLinks_set = sorted(foodLinks_set) # 依順序排序\n",
    "foodLinks_csv = list(sorted(foodLinks_csv))\n",
    "\n",
    "# 儲存檔案\n",
    "# 如果沒有該檔案名，創資料夾\n",
    "dn = \"fatsecret_food\"\n",
    "if not os.path.exists(dn):   \n",
    "    os.makedirs(dn)    \n",
    "\n",
    "\n",
    "# 將foodLinks_scv存檔\n",
    "print(\"食物名連結存檔中...\")\n",
    "\n",
    "my_df = pd.DataFrame(foodLinks_set)\n",
    "my_df.to_csv(dn + \"/\" + 'WebLinks_fatsecret_2_foodLinks_set', index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "my_df = pd.DataFrame(foodLinks_csv)\n",
    "my_df.to_csv(dn + \"/\" + 'WebLinks_fatsecret_2_foodLinks_csv', index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"食物名連結存檔（csv）完成\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三部分：得第三層可能隱藏之連結"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 範例連結：\n",
    "僅有預設連結（內無其他\"食用量\"之連結）：　\n",
    "    1. 有品牌：\n",
    "https://www.fatsecret.cn/%E7%83%AD%E9%87%8F%E8%90%A5%E5%85%BB/%E7%A7%91%E8%BF%AA/%E7%BA%AF%E7%89%9B%E5%A5%B6100/100%E6%AF%AB%E5%8D%87\n",
    "連結內有其他\"食用量\"連結 ：\n",
    "    1. 無品牌：\n",
    "https://www.fatsecret.cn/%E7%83%AD%E9%87%8F%E8%90%A5%E5%85%BB/%E6%99%AE%E9%80%9A%E7%9A%84/%E6%B3%95%E5%9B%BD%E5%8D%A1%E9%97%A8%E5%9F%B9%E5%B0%94(camembert)%E5%A5%B6%E9%85%AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intakeLinks_set = set() # 設起始\"食用量\"連結為空字串\n",
    "errorLinks_set = set() # 設\"食物名\"連結內為錯誤，為空字串\n",
    "\n",
    "for iLink in foodLinks_set:\n",
    "    try: \n",
    "        foodLinks = iLink\n",
    "        response = requests.get(foodLinks)\n",
    "        html = BeautifulSoup(response.text)  \n",
    "        html_intakeLinks = html.find_all(\"a\", href = re.compile(\"portionamount=\")) #解析網頁\n",
    "        # intakeLinks = \"https://www.fatsecret.cn/\" + html_intakeLinks[0][\"href\"] # 取得\"食物名\"連結\n",
    "        \n",
    "\n",
    "        if len(html_intakeLinks)  == 0: # foodLink 內僅有預設\"食用量\"連結\n",
    "\n",
    "            html = BeautifulSoup(response.text) \n",
    "            brand = html.find_all(\"div\", class_ = \"breadcrumb_link\") # 該食物有品牌，為[2]；若無，為\"\"\n",
    "            size =  html.find_all(\"div\", class_ = \"serving_size_value\") # 食用量[0]\n",
    "            intake =  html.find_all(\"div\", class_ = \"breadcrumb_noLink\") # 食物名\n",
    "            \n",
    "            # 以len(brand判斷該食物是否有\"品牌\"\n",
    "            if len(brand) == 3: # 有品牌\n",
    "                intakeLinks_set.add((brand[2].text, intake[0].text, size[0].text, foodLinks))\n",
    "                print(\"品牌：\"+ brand[2].text + \"，食物名：\" + intake[0].text + \n",
    "                      \"，食用量：\" + size[0].text + \"，網址：\", foodLinks)\n",
    "\n",
    "            else: # 無\"品牌\n",
    "                intakeLinks_set.add((\"NULL\", intake[0].text, size[0].text, foodLinks))\n",
    "                print(\"品牌：\"+ \"NULL\" + \"，食物名：\" + intake[0].text + \n",
    "                      \"，食用量：\" + size[0].text + \"，網址：\",foodLinks)\n",
    "                    \n",
    "        if len(html_intakeLinks) != 0:\n",
    "\n",
    "            for i in range (0, len(html_intakeLinks), 2): #\"食用量\"與\"卡路里\"為同樣連結，因此跳2\n",
    "\n",
    "                intakeLinks = \"https://www.fatsecret.cn/\" + html_intakeLinks[i][\"href\"] # 取得\"食物名\"連結\n",
    "                response = requests.get(intakeLinks)\n",
    "                html = BeautifulSoup(response.text) \n",
    "\n",
    "                brand = html.find_all(\"div\", class_ = \"breadcrumb_link\") # 該食物有品牌，為[2]；若無，為\"\"\n",
    "                size =  html.find_all(\"div\", class_ = \"serving_size_value\") # 食用量[0]\n",
    "                intake =  html.find_all(\"div\", class_ = \"breadcrumb_noLink\") # 食物名\n",
    "\n",
    "                if len(brand) == 3: # 該食物\"有\"品牌\n",
    "\n",
    "                    intakeLinks_set.add((brand[1].text, iintake[0].text, size[0].text , intakeLinks))\n",
    "                    print(\"品牌：\"+ brand[2].text + \"，食物名：\" + intake[0].text + \n",
    "                          \"，食用量：\" + size[0].text + \"，網址：\", intakeLinks)  \n",
    "\n",
    "                else: # 該食物\"無\"品牌\n",
    "\n",
    "                    intakeLinks_set.add((\"NULL\", intake[0].text, size[0].text , intakeLinks))\n",
    "                    print(\"品牌：\"+ \"NULL\" + \"，食物名：\" + intake[0].text + \n",
    "                          \"，食用量：\" + size[0].text + \"，網址：\", intakeLinks)  \n",
    "\n",
    "    except:\n",
    "        errorLinks_set.add((iLink))\n",
    "\n",
    "# 有錯誤連結則印，無也印        \n",
    "if len(errorLinks_set) != 0: \n",
    "    for i in errorLinks_set:\n",
    "        print((\"錯誤連結：\", list(i))\n",
    "else:\n",
    "    print(\"沒有錯誤連結\")\n",
    "\n",
    "# 儲存檔案\n",
    "# 如果沒有該檔案名，創資料夾\n",
    "dn = \"fatsecret_food\"\n",
    "if not os.path.exists(dn):   \n",
    "    os.makedirs(dn)    \n",
    "\n",
    "# 將食用量連結連結存檔\n",
    "print(\"食用量連結存檔中...\")\n",
    "\n",
    "intakeLinks_df = pd.DataFrame(intakeLinks_set)\n",
    "intakeLinks_df.to_csv(dn + \"/\" + 'WebLinks_fatsecret_3_intakeLinks_csv', index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"食用量連結存檔（csv）完成\")\n",
    "\n",
    "    \n",
    "# 將錯誤連結存檔\n",
    "print(\"錯誤連結存檔中...\")\n",
    "\n",
    "errorLinks_df = pd.DataFrame(errorLinks_set)\n",
    "errorLinks_df.to_csv(dn + \"/\" + 'WebLinks_fatsecret_3_errorLinks_scv', index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"錯誤連結存檔（csv）完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 第四部分：爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set()要轉換成list()方能套入引數\n",
    "intakeLinks_set = list(intakeLinks_set)\n",
    "\n",
    "print(\"創建空字典\")\n",
    "# 設個空字串將所有食物字典塞入\n",
    "final_dic = []\n",
    "\n",
    "# 將食物名稱，營養成分設個空字典\n",
    "saved = {} # 食物名稱字典\n",
    "nutrient_dic = {} # 營養成分字典\n",
    "\n",
    "\n",
    "\n",
    "for link in range(len(intakeLinks_set)):\n",
    "    \n",
    "    print(\"抓取食物名，食用量，營養成分等...\") #檢查哨 \n",
    "    print(\"\")\n",
    "    url = intakeLinks_set[link][3] #在set的第三項\n",
    "    response = requests.get(url)  # 輸入cookie及內容\n",
    "    html = BeautifulSoup(response.text)\n",
    "\n",
    "    ## 抓品牌，食物名，食用量，營養成分等\n",
    "    # 品牌 \n",
    "    brand = html.find_all(\"div\", class_ = \"breadcrumb_link\") # 該食物有品牌，為[2]；若無，為\"\"\n",
    "    if len(brand) == 3: # 該食物\"有\"品牌\n",
    "        braned = brand[2].text\n",
    "        print(\"品牌：\"+ brand[2].text)\n",
    "              \n",
    "    else: # 該食物\"無\"品牌\n",
    "        braned = \"NULL\"\n",
    "        print(\"品牌：\"+ \"NULL\" )\n",
    "    \n",
    "    \n",
    "    # 食物名 \n",
    "    name = html.find_all(\"div\", class_=\"breadcrumb_noLink\")\n",
    "    print(\"食物名: \"+name[0].text) # 檢查哨  \n",
    "\n",
    "    # 食用量\n",
    "    intake = html.find_all(\"div\", class_=\"serving_size\")\n",
    "\n",
    "    for i in range(len(intake)): # 跑回圈只是為了格式好看 # 檢查哨\n",
    "        print(intake[i].text, end = \" \")\n",
    "\n",
    "    print(\" \") # 為格式好看\n",
    "\n",
    "    # 營養成分 # 印出營養成分 # 檢查哨\n",
    "    nutrient = html.find_all(\"div\", class_=\"nutrient\")\n",
    "    for n in range(1, 3, 2):\n",
    "        print(\"-\" + nutrient[n].text, nutrient[n+1].text) # -為格式好看\n",
    "\n",
    "    # ## 300 千卡的前一項(3)為空值，補上“卡路里”\n",
    "    # 補上卡路里、印出卡路里\n",
    "    print(\"-\" + \"卡路里 \",nutrient[4].text) # -為格式好看\n",
    "    \n",
    "    # 印出其他營養成分\n",
    "    for n in range(5, len(nutrient), 2):\n",
    "        print(\"-\" + nutrient[n].text, nutrient[n+1].text) # -為格式好看\n",
    "    \n",
    "    # 印出更新時間，並將時間轉換成8碼，如20190911\n",
    "    time = html.find(\"div\", style =\"margin-top:5px\").find(\"div\", class_ = \"smallText\").text\n",
    "    timeDrop = time[8:19].replace('年','').replace('月','').replace('日','')\n",
    "    print(\"更新時間：\", timeDrop)\n",
    "    print(\"資訊來源:fatsecret中国\")\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "    # 先整合成一個物件\n",
    "    # 食物名先創字典 (saved)\n",
    "\n",
    "    print(\"創字典...\") #檢查哨\n",
    "    print(\"\")\n",
    "\n",
    "    saved = {\"品牌\":braned ,\n",
    "             \"食物名\":name[0].text,\n",
    "             intake[0].text:intake[1].text,\n",
    "            }\n",
    "    \n",
    "    # 創空字典\n",
    "    nutrient_dic= {} \n",
    "    \n",
    "    # update營養成分到字典\n",
    "\n",
    "    for i in range (1, 3, 2):\n",
    "        dic = {nutrient[i].text: nutrient[i+1].text} \n",
    "        nutrient_dic.update(dic)\n",
    "        \n",
    "    # update卡路里到字典\n",
    "    dic2 = {\"熱量\":nutrient[4].text}\n",
    "    nutrient_dic.update(dic2)\n",
    "    \n",
    "    # update其他營養成分到字典\n",
    "    for n in range(5, len(nutrient), 2):\n",
    "        dic3 = {nutrient[n].text:nutrient[n+1].text}\n",
    "        nutrient_dic.update(dic3)\n",
    "\n",
    "    # update\"資料來源：fatsecret中國\"到字典\n",
    "    \n",
    "    dic_source= {\"資訊來源\":\"fatsecret中国\"}\n",
    "    nutrient_dic.update(dic_source)\n",
    "    \n",
    "    # update\"更新時間：timeDrop\"到字典\n",
    "    \n",
    "    dic_time= {\"更新時間\": timeDrop}\n",
    "    nutrient_dic.update(dic_time)\n",
    "        \n",
    "        \n",
    "    # update到最初的字典\n",
    "    saved.update(nutrient_dic)\n",
    "    \n",
    "    print(\"該食物加入字典完成\")\n",
    "    print(\"\")\n",
    "    final_dic.append((saved))\n",
    "    \n",
    "print(\"=\"*90)    \n",
    "print(\"字典創建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五部分：存檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 存檔\n",
    "\n",
    "print(\"存檔中...\") #檢查哨\n",
    "print(\"\")\n",
    "\n",
    "dn = \"fatsecret_food\"\n",
    "fn = \"fatsecret_foodData\"\n",
    "if not os.path.exists(dn):   \n",
    "    os.makedirs(dn)\n",
    "if not os.path.exists(dn+\"/\"+fn):   \n",
    "    os.makedirs(dn+\"/\"+fn)\n",
    "    \n",
    "# 存json格式\n",
    "f = open(dn + \"/\"+ fn +\"/\"+ \"Data_fatsecret\" + \".json\", \"w\", encoding=\"utf-8\") # 打開路徑\n",
    "json.dump(final_dic, f) # 寫入json格式\n",
    "f.close() # 關閉路徑\n",
    "print(\"json存檔完成\")\n",
    "print(\"\")\n",
    "\n",
    "# 存csv格式\n",
    "final_dic_df = pd.DataFrame(final_dic)\n",
    "final_dic_df.to_csv(dn + \"/\"+ fn +\"/\"+ \"Data_fatsecret\" + \".csv\", index=False, header=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"csv存檔完成\")\n",
    "print(\"\")\n",
    "\n",
    "# # DONE\n",
    "print(\"存檔完成\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"Done!\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 備用\n",
    "#### 若不幸於第一~三部分中斷，可直接讀特定部分的csv檔，以利續爬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "### 開啟 CSV 檔案\n",
    "with open('fatsecret_food\\\\3_fatsecret_intakeLinks_csv', newline='', encoding=\"utf-8\") as csvfile:\n",
    "\n",
    "  ### 讀取 CSV 檔案內容\n",
    "    rows = csv.reader(csvfile)\n",
    "    intakeLinks_set = [] # 設空字串，以便後續爬\n",
    "    \n",
    "    # 以迴圈輸出每一列\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        intakeLinks_set.append(row) # 將csv每行逐一加入字串\n",
    "\n",
    "for i in range (len(intakeLinks_set)):\n",
    "    print(i, intakeLinks_set[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
